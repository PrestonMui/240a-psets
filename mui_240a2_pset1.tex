\documentclass{article}[12pt]
\usepackage{enumerate,amsfonts,graphicx,amsthm,amssymb,graphicx}
\usepackage[mathscr]{euscript}
\usepackage[hmargin=1.5cm,vmargin=1.5cm]{geometry}
\usepackage[fleqn]{amsmath}

% \setlength{\mathindent}{15pt}

% BEGIN DOCUMENT

\begin{document}

\begin{center}
	Preston Mui | mui@berkeley.edu

	{\bf Econ-240a: Pset 1}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Problems
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcounter{section}{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 2
\section{Binomial-Beta Learning}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{enumerate}
	\item \textbf{What is the conditional distribution of $Z_N$ given $\theta$?}
	\begin{align*}
	    f_{Z_N | \Theta}(z | \theta) &= \begin{cases}
	    \binom{n}{z} \theta^z (1-\theta)^{N - z} &\mbox{ if $z \in 0, 1, \cdots, N$} \\
	    0 &\mbox{ otherwise}
	    \end{cases}
	\end{align*}

	\item \textbf{Calculate the joint distribution of $Z_N$ and $\theta$}
	\begin{align*}
	    f_{Z_N,\Theta}(z,\theta) &= f_{\Theta}(\theta) f_{Z_N | \Theta}(z|\theta) = \begin{cases} \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \binom{n}{z} \theta^z (1-\theta)^{N - z} \mbox{ if $z \in 0, 1, \cdots, N$ and $\theta \in [0,1]$}\\ 0 \mbox{ otherwise} \end{cases}
	\end{align*}

	\item \textbf{Calculate the conditional distribution of $\theta$ given $Z_N$. What is the mean of this distribution, and why would you call it Posterior?} By Bayes' Rule,
	\begin{align*}
	    f_{\Theta | Z_N}(\theta | z) &= \frac{f_{\Theta, Z_N}(\theta,z)}{f_{Z_N}}\\
	    &= \frac{\frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \binom{n}{z} \theta^z (1-\theta)^{N - z}}{\int_{0}^{1} \frac{\theta^{a-1}(1-\theta)^{b-1}}{B(a,b)} \binom{n}{z} \theta^z (1-\theta)^{N - z} d\theta}\\
	    &= \frac{\theta^{a-1}(1-\theta)^{b-1} \theta^z (1-\theta)^{N - z}}{\int_{0}^{1} \theta^{z+a-1}(1-\theta)^{N-z+b-1} d\theta} = \frac{\theta^{z+a-1}(1-\theta)^{N-z+b-1}}{B(z+a,N-z+b)}
	\end{align*}
	So $\Theta | Z_N$ is beta-distributed $(z+a,N-z+b)$, with a mean of $\frac{z+a}{z+a+N-z+b} = \frac{z+a}{N+a+b}$. The name is ``posterior'' because it is finding a distribution of $\theta$ \textbf{after}, or ``post'' drawing the data.

	\item \textbf{Say $\alpha = \beta = 1/2$.}
	
	This prior assigns the bulk of the prior belief to the tails (close to 0 and 1) with low probability for $\theta$ close to 0.5, and even probability to $\theta > 0.5$ and $\theta < 0.5$. This prior is not very good because almost assuredly more than half of Cal students would intend to vote for Obama, so assigning equal probability mass to both sides of $0.5$ does not make sense. Also, a belief that assigns high probability mass to \emph{both} large support for Obama and very little support for Obama does not make sense.

	\item \textbf{The Harold Jeffreys interval:} This interval uses the $\frac{\alpha}{2}$ and $1 - \frac{\alpha}{2}$ quantiles of the posterior distribution from above. So, given the prior distribution on $\theta$ from above, the Harold Jeffreys interval contains the true value of $\theta$ with probability $1-\alpha$.

	\item \textbf{Given a Jeffrey's interval of [0.47,0.73], what is the probability that this interval contains $\theta$?} If I have constructed a Jeffrey's interval for some prior distribution of $\theta$, some data sample, and some choice of $\alpha$, then I would say that the interval contains $\theta$ is $1-\alpha$ with probability $1-\alpha$. Of course, in \emph{reality}, the probability is either 0 or 1, because the ``true'' $\theta$ is fundamentally deterministic. This probability reflects some probabilistic belief about $\theta$ that depends on the choice of prior.

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Section 3
\section{Multivariate Normal Distribution}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{enumerate}
	\item \textbf{Let $C$ be a $K \times K$ nonsingular matrix. Show that $Z = CY$ is distributed according to $\mathscr{N} (C \mu,C \Sigma C')$}:

	Because $Z = CY$ is a linear transformation, and $C$ is invertible,
	\begin{align*}
		f(z_1, \cdots, z_K) &= \frac{1}{|C|} f[C^{-1}Y] \\
		&= (2 \pi)^{-K/2} |\Sigma|^{-1/2} |C|^{-1} \exp \bigg( -\frac{1}{2}(C^{-1}y - \mu)' \Sigma^{-1} (C^{-1}y - \mu) \bigg) \\
		&= (2 \pi)^{-K/2} |\Sigma|^{-1/2} |C|^{-1/2} |C|^{-1/2} \exp \bigg( -\frac{1}{2}(C^{-1}y - C^{-1}C\mu)' \Sigma^{-1} (C^{-1}y - C^{-1}C\mu) \bigg) \\
		&= (2 \pi)^{-K/2} (|C||\Sigma||C|)^{-1/2} \exp \bigg( -\frac{1}{2}(C^{-1}(y - C\mu))' \Sigma^{-1} (C^{-1}(y - C\mu)) \bigg) \\
		&= (2 \pi)^{-K/2} (|C\Sigma C'|)^{-1/2} \exp \bigg( -\frac{1}{2}(y - C\mu)' C^{-1'} \Sigma^{-1} C^{-1}(y - C\mu) \bigg) \\
		&= (2 \pi)^{-K/2} (|C\Sigma C'|)^{-1/2} \exp \bigg( -\frac{1}{2}(y - C\mu)' C'^{-1} \Sigma^{-1} C^{-1}(y - C\mu) \bigg) \\
		&= (2 \pi)^{-K/2} (|C\Sigma C'|)^{-1/2} \exp \bigg( -\frac{1}{2}(y - C\mu)' (C \Sigma C')^{-1}(y - C\mu) \bigg)
	\end{align*}
	Which is a distribution with mean $C\mu$ and variance-covariance matrix $C\Sigma C'$.

	\item Show that $Y_1', Y_2'$ are independent if $\Sigma_{12} = \Sigma'_{21} = \underline{00}'$.

	If $Y_1'$  and $Y_2'$ are independent, then $f(Y_1')f(Y_2') = f(Y_1',Y_2')$:
	\begin{align*}
		f(Y_1',Y_2') &= (2\pi)^{-K/2} |\Sigma|^{-1/2} \exp \bigg( -\frac{1}{2} (y - \mu)' \Sigma^{-1} (y - \mu) \bigg)\\
		&= (2\pi)^{-(K_1+K_2)/2} (|\Sigma_{11}||\Sigma_{22}|)^{-1/2} \exp \bigg(-\frac{1}{2} (y_1-\mu_1, y_2 - \mu_2)' \begin{pmatrix} (\Sigma_{11})^{-1} & \underline{00}' \\ \underline{00} & (\Sigma_{22})^{-1} \\ \end{pmatrix} (y_1-\mu_1, y_2 - \mu_2) \bigg) \\
			& \mbox{using the following facts:}\\
			& \Sigma_{12} = \Sigma_{21} = \underline{00}' \implies |\Sigma|=|\Sigma_{11}| |\Sigma_{22}| \\
			& \Sigma_{12} = \Sigma_{21} = \underline{00}' \implies \Sigma^{-1} =
				\begin{pmatrix}
					(\Sigma_{11})^{-1} & \underline{00}' \\
					\underline{00} & (\Sigma_{22})^{-1} \\
				\end{pmatrix}\\
		&= (2\pi)^{-(K_1+K_2)/2} (|\Sigma_{11}||\Sigma_{22}|)^{-1/2} \exp \bigg(-\frac{1}{2} \big((y_1-\mu_1)'(\Sigma_{11})^{-1}, (y_2 - \mu_2)' \Sigma_{22})^{-1} \big) \big(y_1-\mu_1, y_2 - \mu_2\big) \bigg) \\
		&= (2\pi)^{-(K_1+K_2)/2} (|\Sigma_{11}||\Sigma_{22}|)^{-1/2} \exp \bigg(-\frac{1}{2} \big((y_1-\mu_1)'(\Sigma_{11})^{-1}(y_1-\mu_1) + (y_2 - \mu_2)' \Sigma_{22})^{-1} (y_2 - \mu_2) \big) \bigg) \\
		&= (2\pi)^{-K_1/2} |\Sigma_{11}|^{-1/2} \exp \bigg(-\frac{1}{2} (y_1-\mu_1)'(\Sigma_{11})^{-1}(y_1-\mu_1) \bigg) \times \cdots \\
			& \mbox{\hspace{3cm}} \cdots (2\pi)^{-K_2/2} |\Sigma_{22}|^{-1/2} \exp \bigg(-\frac{1}{2} (y_2 - \mu_2)' (\Sigma_{22})^{-1} (y_2 - \mu_2) \bigg) = f(Y_1')f(Y_2')
	\end{align*}
	Where $Y_1$ and $Y_2$ are independently distributed with means $\mu_1, \mu_2$ and variance-covariance matrices $\Sigma_{11}, \Sigma_{22}$.

	\item From part 1, $Z = CY$ is distributed normally with mean $C \mu$ and variance-covariance matrix $C \Sigma C'$:
	\begin{align*}
		C \mu &=
			\begin{pmatrix} I_{K_1} & -\Sigma_{12}\Sigma_{22}^{-1} \\ 0 & I_{K_2} \end{pmatrix}
			\begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}
		= \begin{pmatrix} \mu_1 - \Sigma_{12} \Sigma_{22}^{-1} \mu_2 \\ \mu_2 \end{pmatrix} \\
		C \Sigma C' &=
			\begin{pmatrix} I_{K_1} & -\Sigma_{12}\Sigma_{22}^{-1} \\ 0 & I_{K_2} \end{pmatrix}
			\begin{pmatrix} \Sigma_{11} &\Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
			\begin{pmatrix} I_{K_1} & 0 \\ (-\Sigma_{12}\Sigma_{22}^{-1})' & I_{K_2} \end{pmatrix} \\
		&=	\begin{pmatrix} \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{12} - \Sigma_{12}\Sigma_{22}^{-1} \Sigma_{22} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
			\begin{pmatrix} I_{K_1} & 0 \\ -(\Sigma_{22}^{-1})' \Sigma_{12}' & I_{K_2} \end{pmatrix} \\
		&=	\begin{pmatrix} \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & 0 \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
			\begin{pmatrix} I_{K_1} & 0 \\ -\Sigma_{22}^{-1} \Sigma_{21} & I_{K_2} \end{pmatrix} \\
		&=	\begin{pmatrix} \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & 0 \\ \Sigma_{21} - \Sigma_{22}\Sigma_{22}^{-1}\Sigma_{21} & \Sigma_{22} \end{pmatrix} \\
		&=	\begin{pmatrix} \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} & 0 \\ 0 & \Sigma_{22} \end{pmatrix}
	\end{align*}

	\item Let $E$ be the matrix $\begin{pmatrix} 0 & I_{K-P} \end{pmatrix}$, which is the $(K-P) \times P$ zero matrix concatenated with the $(K-P) \times (K-P)$ identity matrix. From previous results we know that $\begin{pmatrix} D \\ E \end{pmatrix} Y$ is distributed $\mathscr{N} \bigg( \begin{pmatrix} D \\ E \end{pmatrix} \mu, \begin{pmatrix} D \\ E \end{pmatrix} \Sigma \begin{pmatrix} D \\ E \end{pmatrix}' \bigg)$.

	The mean of $\begin{pmatrix} D \\ E \end{pmatrix} Y$ is
	\begin{align*}
		\begin{pmatrix} D_{11} & D_{12} \\ 0 & I_{K-P,K-P} \end{pmatrix} \mu &= \begin{pmatrix} D\mu \\ \mu_2 \end{pmatrix}
	\end{align*}
	and the variance of $\begin{pmatrix} D \\ E \end{pmatrix} Y$ is
	\begin{align*}
		&\begin{pmatrix} D_{11} & D_{12} \\ 0 & I_{K-P,K-P} \end{pmatrix}
			\begin{pmatrix} \Sigma_{11} & \Sigma_{12} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
			\begin{pmatrix} D_{11}' & 0 \\ D_{12}' & I_{K-P,K-P} \end{pmatrix} \\
		&= \begin{pmatrix} D_{11}\Sigma_{11} + D_{12}\Sigma_{21} & D_{11}\Sigma_{12} + D_{12}\Sigma_{22} \\ \Sigma_{21} & \Sigma_{22} \end{pmatrix}
			\begin{pmatrix} D_{11}' & 0 \\ D_{12}' & I_{K-P,K-P} \end{pmatrix} \\
		&= \begin{pmatrix} D_{11}\Sigma_{11}D_{11}' + D_{12}\Sigma_{21}D_{11}' + D_{11}\Sigma_{12}D_{12}' + D_{12}\Sigma_{22}D_{12}' & D_{11}\Sigma_{12} + D_{12}\Sigma_{22} \\
		\Sigma_{21}D_{11}' + \Sigma_{22}D_{12}' & \Sigma_{22} \end{pmatrix} \\
		&= \begin{pmatrix} D \Sigma D' & D_{11}\Sigma + D_{12}\Sigma_{22} \\ \Sigma_{21}D_{11}' + \Sigma_{22}D_{12}' & \Sigma_{22} \end{pmatrix}
	\end{align*}
	Taking the first $P$ elements of $\begin{pmatrix} D\mu \\ \mu_2 \end{pmatrix}$ and the upper-left $P \times P$ elements of $\begin{pmatrix} D \Sigma D' & D_{11}\Sigma + D_{12}\Sigma_{22} \\ \Sigma_{21}D_{11}' + \Sigma_{22}D_{12}' & \Sigma_{22} \end{pmatrix}$ shows that $Z$ is distributed normally with mean $D \mu$ and variance-covariance $D \Sigma D'$.

	\item Let $C = \begin{pmatrix} I_{K_1} & -\Sigma_{12}\Sigma_{22}^{-1} \\ 0 & I_{K_2} \end{pmatrix}$ from question 3, where I found that $(CY)_1$ and $(CY)_2$ are independent, where $(CY)_1$ is the first $K_1$ elements of $CY$ and $(CY)_2$ the last $K_2$ elements of $CY$. In question 4 I found that $(CY)_1$ is distributed $\mathscr{N} \bigg( \mu_1 - \Sigma_{12}\Sigma_{22}^{-1}\mu_2, \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \bigg)$. Since $(CY)_1$ and $(CY)_2$ are independent, and $(CY)_2 = Y_2$, this is also the conditional distribution of $(CY)_1$ given $Y_2$. That is,
	\begin{align*}
		\bigg( (Y_1 - \Sigma_{12}\Sigma_{22}^{-1} Y_2) | Y_2 = y_2 \bigg) &\sim \mathscr{N} \bigg( \mu_1 - \Sigma_{12}\Sigma_{22}^{-1}\mu_2, \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \bigg) \\
		\bigg( Y_1 | Y_2 = y_2 \bigg) &\sim \Sigma_{12}\Sigma_{22}^{-1} y_2 + \mathscr{N} \bigg( \mu_1 - \Sigma_{12}\Sigma_{22}^{-1}\mu_2, \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \bigg) \\
		\bigg( Y_1 | Y_2 = y_2 \bigg) &\sim \mathscr{N} \bigg( \mu_1 - \Sigma_{12}\Sigma_{22}^{-1} (y_2 - \mu_2), \Sigma_{11} - \Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} \bigg)
	\end{align*}

	\item Let $D = \begin{pmatrix} I_{K \times K} ^{(1)} & I_{K \times K} ^{(2)} & \cdots & I_{K \times K} ^{(N)}  \end{pmatrix}$; that is, $D$ is constructed by concatenating $N$ $K \times K$ identity matrices. Then,let $\mathbb{Y} = \begin{pmatrix} Y_1 & Y_2 & \cdots & Y_N \end{pmatrix}'$; that is, $\mathbb{Y}$ is constructed by stacking the $N$ independent random draws of $Y_i$ stacked together. Note that since each $Y_i$ is independently, identically, and normally distributed with mean $\mu$ and variance-covariance matrix $\Sigma$, $\mathbb{Y}$ is normally distributed with mean $\mu_{\mathbb{Y}}$ and $\Sigma_{\mathbb{Y}}$ such that 
	\begin{align*}
		\mu_{\mathbb{Y}} &=	\begin{pmatrix} \mu^{(1)} \\ \mu^{(2)} \\ \vdots \\ \mu^{(N)} \end{pmatrix} \\
		\Sigma_{\mathbb{Y}} &=
			\begin{pmatrix}
				\Sigma^{(1)} & 0_{K \times K} & \cdots & 0_{K \times K} \\
				0_{K \times K} & \Sigma^{(2)} & \cdots & 0_{K \times K} \\
				\vdots & \vdots & \ddots & \vdots \\
				0_{K \times K} & \cdots & 0_{K \times K} & \Sigma^{(N)}
			\end{pmatrix}
	\end{align*}
	where $\mu^{(i)} = \mu$ and $\Sigma^{(i)} = \Sigma$ for each $i = 1, \cdots, N$.

	Then, using the result from problem 4, I can construct $\begin{pmatrix} D \\ E \end{pmatrix}$, an $NK \times NK$ square matrix such that the first $K$ elements of $\begin{pmatrix} D \\ E \end{pmatrix} \mathbb{Y}$, $D\mathbb{Y}$, are distributed $\mathscr{N} \bigg( D \mu_{\mathbb{Y}}, D \Sigma_{\mathbb{Y}} D' \bigg)$.

	Let $Y_{ij}$ be the $j$th element of the $i$th random vector. Then, examine $D \mathbb{Y}$:
	\begin{align*}
		D \mathbb{Y} &= \begin{pmatrix} \sum_{i=1}^N Y_{i1} & \sum_{i=1}^N Y_{i2} & \cdots & \sum_{i=1}^N Y_{iK} \end{pmatrix}' = \sum_{i=1}^N Y_i
	\end{align*}
	That is, $D \mathbb{Y}$ is the sum of the $N$ random draws of $Y_i$. This sum is distributed with mean:
	\begin{align*}
		D \mu_{\mathbb{Y}} &= \begin{pmatrix} \sum_{i=1}^N \mu_1 & \sum_{i=1}^N \mu_2 & \cdots & \sum_{i=1}^N \mu_K \end{pmatrix}' = N \mu
	\end{align*}
	and variance:
	\begin{align*}
		D \Sigma_{\mathbb{Y}} D' &=
			\begin{pmatrix} I_{K \times K} ^{(1)} & I_{K \times K} ^{(2)} & \cdots & I_{K \times K} ^{(N)} \end{pmatrix}
			\begin{pmatrix}
				\Sigma^{(1)} & 0_{K \times K} & \cdots & 0_{K \times K} \\
				0_{K \times K} & \Sigma^{(2)} & \cdots & 0_{K \times K} \\
				\vdots & \vdots & \ddots & \vdots \\
				0_{K \times K} & \cdots & 0_{K \times K} & \Sigma^{(N)}
			\end{pmatrix}
			\begin{pmatrix} I_{K \times K} ^{(1)} & I_{K \times K} ^{(2)} & \cdots & I_{K \times K} ^{(N)}  \end{pmatrix}' \\
		&= \begin{pmatrix} \Sigma^{(1)} & \Sigma^{(2)} & \cdots & \Sigma^{(N)} \end{pmatrix} \begin{pmatrix} I_{K \times K} ^{(1)} & I_{K \times K} ^{(2)} & \cdots & I_{K \times K} ^{(N)}  \end{pmatrix}' = N \Sigma\\
	\end{align*}
	That is, $\sum_{i=1}^N Y_i$ is distributed $\mathscr{N}\bigg( N\mu , N\Sigma \bigg)$. Using the result from part 1,
	\begin{align*}
		& \overline{Y_i} = \frac{1}{N} \sum_{i=1}^N Y_i = \frac{1}{N} I_{N \times N} \sum_{i=1}^N Y_i
			\sim \mathscr{N}\bigg( \frac{1}{N} I_{N \times N} N\mu , (\frac{1}{N} I_{N \times N}) N\Sigma (\frac{1}{N} I_{N \times N})' \bigg) = \mathscr{N}\bigg( \mu , \frac{1}{N} \Sigma \bigg) \\
		\implies & \overline{Y_i} - \mu \sim \mathscr{N}\bigg(0, \frac{1}{N} \Sigma \bigg) \\
		\implies & \sqrt{N} (\overline{Y_i} - \mu) \sim \mathscr{N}\bigg( \sqrt{N} 0, \sqrt{N}^2 \frac{1}{N} \Sigma \bigg) = \mathscr{N}\bigg( 0, \Sigma \bigg)
	\end{align*}

	\item Because $\Sigma$ is a symmetric and positive definite matrix, there exists a unique and symmetric $\sqrt{\Sigma}$ such that $\sqrt{\Sigma}\sqrt{\Sigma} = \Sigma$. Then, examine $W$:
	\begin{align*}
		W &= N (\bar{Y} - \mu)' \Sigma^{-1} (\bar{Y} - \mu) \\
		&= \sqrt{N} (\bar{Y} - \mu)' \Sigma^{-1} \sqrt{N} (\bar{Y} - \mu)
	\end{align*}
	Let $Z \sim \mathscr{N}(0,I)$; that is, $Z$ is a standard multivariate normal. Above we found that $\sqrt{N} (\bar{Y} - \mu) \sim \mathscr{N}(0,\Sigma)$; using the result from question 1 and the aforementioned properties of $\Sigma,$ we can say that $\sqrt{\Sigma} Z \sim \mathscr{N}(0,\Sigma)$. So, $W$ can be rewritten as
	\begin{align*}
		W &= (\sqrt{\Sigma} Z)' \Sigma^{-1} (\sqrt{\Sigma} Z) \\
		&= Z'\sqrt{\Sigma}' (\sqrt{\Sigma}\sqrt{\Sigma})^{-1} \sqrt{\Sigma} Z \\
		&= Z'\sqrt{\Sigma} \sqrt{\Sigma}^{-1} \sqrt{\Sigma}^{-1} \sqrt{\Sigma} Z = Z' Z
	\end{align*}
	which is distributed as $\chi^2_{K}$.

	\item Recall from the process of solving question 6 that I found that $\bar{Y_i} \sim \mathscr{N}(\mu,\frac{1}{N}\Sigma)$. Maintaining $H_0$ and using the result from question 4, the distribution of $D\bar{Y}$ is given by $\mathscr{N}(D\mu, \frac{1}{N} D\Sigma D')$.

	The distribution of $W$ is given by 
	\begin{align*}
		W &= N (D \bar{Y} - d)' (D\Sigma D')^{-1} (D\bar{Y} - d) \\
		&= N (D \bar{Y} - D\mu)' (D\Sigma D')^{-1} (D\bar{Y} - D\mu) \\
		&= N (D(\bar{Y} - \mu))' (D\Sigma D')^{-1} D(\bar{Y} - \mu) \\
		&= N (\bar{Y} - \mu)' D' D'^{-1} \Sigma^{-1} D^{-1} D(\bar{Y} - \mu) \\
		&= N (\bar{Y} - \mu)' \Sigma^{-1} (\bar{Y} - \mu) \sim \chi^2_K \mbox{ from the previous problem.}
	\end{align*}

	If $H_0$ is true, the ex ante probability of $W > \chi_P^{2, 1-\alpha}$ is less than $\alpha = 0.05$. If I observed this $W$ in sample, I would conclude that the observed data given $H_0$ is very unlikely, and reject $H_0$.

	\end{enumerate}
\end{document}